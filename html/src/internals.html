<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Notes on the internal implementation &mdash; Polynomials Manipulation Module v1.0 documentation</title>
    <link rel="stylesheet" href="../static/default.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1.0',
        COLLAPSE_MODINDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../static/jquery.js"></script>
    <script type="text/javascript" src="../static/doctools.js"></script>
    <script type="text/javascript" src="../static/jsMath-3.6e/easy/load.js"></script>
    <link rel="top" title="Polynomials Manipulation Module v1.0 documentation" href="../index.html" />
    <link rel="next" title="Algorithms for algebraic computations" href="algorithms.html" />
    <link rel="prev" title="Introduction" href="introduction.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="algorithms.html" title="Algorithms for algebraic computations"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="introduction.html" title="Introduction"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Polynomials Manipulation Module v1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="notes-on-the-internal-implementation">
<span id="thesis-internals"></span><h1>Notes on the internal implementation<a class="headerlink" href="#notes-on-the-internal-implementation" title="Permalink to this headline">¶</a></h1>
<p>Knowing the goals of the project, lets now focus on the details of internal implementation
of polynomials manipulation module and methods that were used to reach those goals. In this
chapter we will describe step&#8211;by&#8211;step all the details concerning the design and implementation
of the module, from the technical point of view. In the next chapter we will jump in the details
of the implemented algorithms.</p>
<div class="section" id="physical-structure-of-the-module">
<h2>Physical structure of the module<a class="headerlink" href="#physical-structure-of-the-module" title="Permalink to this headline">¶</a></h2>
<p>Polynomials manipulation module <tt class="docutils literal"><span class="pre">sympy.polys</span></tt> consists of a single directory, <tt class="docutils literal"><span class="pre">sympy/polys</span></tt>,
in SymPy&#8216;s source base and contains the following Python source files:</p>
<dl class="docutils">
<dt><tt class="docutils literal"><span class="pre">__init__.py</span></tt></dt>
<dd>Contains imports of the public API.</dd>
<dt><tt class="docutils literal"><span class="pre">algebratools.py</span></tt></dt>
<dd>Definitions of categories and domains.</dd>
<dt><tt class="docutils literal"><span class="pre">densearith.py</span></tt></dt>
<dd>Arithmetics algorithms for dense polynomial representation.</dd>
<dt><tt class="docutils literal"><span class="pre">densebasic.py</span></tt></dt>
<dd>Basic algorithms for dense polynomial representation.</dd>
<dt><tt class="docutils literal"><span class="pre">densetools.py</span></tt></dt>
<dd>Advanced algorithms for dense polynomial representation.</dd>
<dt><tt class="docutils literal"><span class="pre">factortools.py</span></tt></dt>
<dd>Low&#8211;level algorithms for polynomial factorization.</dd>
<dt><tt class="docutils literal"><span class="pre">galoistools.py</span></tt></dt>
<dd>Implementation of univariate polynomials over finite fields.</dd>
<dt><tt class="docutils literal"><span class="pre">groebnertools.py</span></tt></dt>
<dd>Sparse distributed polynomials and Gröbner bases.</dd>
<dt><tt class="docutils literal"><span class="pre">monomialtools.py</span></tt></dt>
<dd>Functions for enumerating and manipulating monomials.</dd>
<dt><tt class="docutils literal"><span class="pre">numberfields.py</span></tt></dt>
<dd>Tools for computations in algebraic number fields.</dd>
<dt><tt class="docutils literal"><span class="pre">orthopolys.py</span></tt></dt>
<dd>Functions for generating orthogonal polynomials.</dd>
<dt><tt class="docutils literal"><span class="pre">polyclasses.py</span></tt></dt>
<dd>OO layer over low&#8211;level polynomial manipulation functions.</dd>
<dt><tt class="docutils literal"><span class="pre">polyconfig.py</span></tt></dt>
<dd>Tools for configuring functionality of the module.</dd>
<dt><tt class="docutils literal"><span class="pre">polycontext.py</span></tt></dt>
<dd>Tools for managing contexts of evaluation.</dd>
<dt><tt class="docutils literal"><span class="pre">polyerrors.py</span></tt></dt>
<dd>Definitions of polynomial specific exceptions.</dd>
<dt><tt class="docutils literal"><span class="pre">polyoptions.py</span></tt></dt>
<dd>Managers of options that can used with the public API.</dd>
<dt><tt class="docutils literal"><span class="pre">polyroots.py</span></tt></dt>
<dd>Algorithms for root finding, specifically via radicals.</dd>
<dt><tt class="docutils literal"><span class="pre">polytools.py</span></tt></dt>
<dd>The main part of the public API of polynomials manipulation module.</dd>
<dt><tt class="docutils literal"><span class="pre">polyutils.py</span></tt></dt>
<dd>Internal utilities for expression parsing, handling generators etc.</dd>
<dt><tt class="docutils literal"><span class="pre">rootisolation.py</span></tt></dt>
<dd>Low&#8211;level algorithms for symbolic real and complex root isolation.</dd>
<dt><tt class="docutils literal"><span class="pre">rootoftools.py</span></tt></dt>
<dd>Tools for formal handling of polynomial roots: <tt class="docutils literal"><span class="pre">RootOf</span></tt> and <tt class="docutils literal"><span class="pre">RootSum</span></tt>.</dd>
<dt><tt class="docutils literal"><span class="pre">specialpolys.py</span></tt></dt>
<dd>A collection of functions for generating special sorts of polynomials.</dd>
</dl>
<p>There are also two subdirectories with tests and benchmarks. Altogether there are about 1900
functions and methods, and about 90 classes in about 30 thousandth lines of code. We do not
give exact measures because those statistics are not that important and are changing all the
time and the module is developed. Also, many features are implemented outside the module, for
example solvers, expression simplification tools and partial fraction decomposition algorithms.</p>
</div>
<div class="section" id="logical-structure-of-the-module">
<h2>Logical structure of the module<a class="headerlink" href="#logical-structure-of-the-module" title="Permalink to this headline">¶</a></h2>
<p>One of the main concerns when designing a symbolic manipulation library, especially one which is
written in an interpreted general purpose programming language, is speed. Even the best equipped
library, with most recent, cutting edge algorithms and data structures, user friendly API and
easily configurable internals, has little value if the user needs to wait ages for any, even
trivial, results. This was recently the main problem with SymPy in general and its fundamental
weakness. It should be clearly understood that code written in an interpreted language will be
always slower than compiled code, unless we had a very clever JIT (Just&#8211;In&#8211;Time) compiler which
could optimize and compile the code on&#8211;the&#8211;fly. There is some progress in this area, especially
within projects Unladen Swallow <a class="reference external" href="literature.html#unladenswallow">[UnladenSwallow]</a> and PyPy <a class="reference external" href="literature.html#pypy">[PyPy]</a>, but we are still waiting for
truly working solutions. This, however, should not be discouraging and we are supposed to put our
best efforts to make SymPy as fast as possible on pure Python level, especially when implementing
the infrastructure which is used everywhere else in the library.</p>
<p>There is a trivial observation about interpreted programming languages: there is a very high cost
associated with every function call and with every use of <em>magic</em> functionality of the language of
choice. This statement is true in the general case of interpreted languages and the first part is
especially true in the case of Python programming language. There is another observation concerning
algorithms of symbolic mathematics: often they require very large number of function calls. The number
varies between different algorithms, but for the most complex ones, the number can be as high as millions
(or more) function calls per algorithm execution.</p>
<p>One obvious (in theory) solution to this problem is to use <em>better</em> algorithms. Unfortunately, it
is not clear what we mean by better in this context. If we take only pure algorithmic complexity
of methods we implement in SymPy, then one could argue that we should always implement polynomial
time algorithms, of course if they exist in the particular area of interest. This would be a perfect
solution, however, an interesting phenomenon occurs. Often polynomial time algorithms are slower for
common input than their counter parts which exhibit exponential time complexity. This is true, for
example, in the case of polynomial factorization algorithms, where LLL algorithm <a class="reference external" href="literature.html#lenstra1982factor">[Lenstra1982factor]</a>,
the only known polynomial time approach to polynomial factorization, is almost always slower than
<em>efficient</em> exponential time algorithms. Besides this phenomenon, whatever approach we take for choosing
a good algorithm for implementation in SymPy for improving speed, there is always a high cost associated
with such development, because first one has to assure correctness of the newly implemented algorithm and
only then think about improving speed. Of course, this is what we do in SymPy and a discussion about
algorithms will follow in the next chapter.</p>
<p>There is, however, another method for significantly improving computations speed. In parallel with
implementation of <em>better</em> algorithms, one can eliminate as much overhead as possible. The overhead
is associated with the cost of interpretation of program code. As we stated before, the cost of
function calls in Python is high and the more <em>advanced</em> constructs of the language we use the
higher the cost is. However, the less <em>magic</em> we use the harder is to use the code, so our task
is to find the cross&#8211;over point, where we have balance between speed and usability. This is
because speed without usability is as much pointless as usability without speed.</p>
<p>The answer to those observations is a design of polynomials manipulation module in a form of a
multiple&#8211;level environment, where on the lowest level are fundamental functions which are run
most often and form a computational basis for other levels which tend to be much more oriented
towards the user, adding the cost associated with more user friendly API. Multiple&#8211;levels is
nothing new to symbolic mathematics and this is how things were done from the very beginning.
However, previously those levels were associated with usage of different programming languages,
where the core was one level, written in a compiled programming language like C or C++, and
usually not accessible by the end user. The other was a library of mathematical algorithms,
written in a domain specific language (DSL), designed specially for a particular symbolic
mathematics system. Contrary, SymPy is written entirely in a single, interpreted programming
language, so we introduce multiple levels in this language by selecting appropriate sets of
feasible features of the language for each level.</p>
<p>In polynomials manipulation module four levels were introduced: L0, L1, L2 and L3. Each level
has its own syntax and API, and is used for different tasks in SymPy. Redundancy between
levels is reduced to minimum, so that not a single algorithm is duplicated on any level. Also
tests were designed the way that they only test correctness of incrementally added functionality.
On the lowest level we test correctness of the implementations of algorithms of mathematics, and
on other levels we test (mostly) APIs and correctness of argument passing. The first two levels,
L0 and L1, are used internally in the module. The other two levels form the public API of the
module and are used extensively in other parts of SymPy and in interactive sessions.</p>
<div class="section" id="motivation">
<span id="thesis-struct-motivation"></span><h3>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h3>
<p>Why we need exactly four levels in polynomials manipulation module? Suppose we need to compute
a factorization of polynomial <span class="math">x^{10} - 1</span>. There are four levels, so we can perform the same
computation in four different ways:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">f3</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">10</span> <span class="o">-</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">factor_list</span><span class="p">(</span><span class="n">f3</span><span class="p">)</span>
<span class="go">100 loops, best of 3: 5.57 ms per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">f2</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">10</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s">&#39;ZZ&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">f2</span><span class="o">.</span><span class="n">factor_list</span><span class="p">()</span>
<span class="go">100 loops, best of 3: 2.15 ms per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">f1</span> <span class="o">=</span> <span class="n">DMP</span><span class="p">([</span><span class="n">mpz</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
<span class="gp">... </span><span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">ZZ</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">f1</span><span class="o">.</span><span class="n">factor_list</span><span class="p">()</span>
<span class="go">100 loops, best of 3: 1.90 ms per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">f0</span> <span class="o">=</span> <span class="p">[</span><span class="n">mpz</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
<span class="gp">... </span><span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">mpz</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">dup_factor_list</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="n">ZZ</span><span class="p">)</span>
<span class="go">100 loops, best of 3: 1.88 ms per loop</span>
</pre></div>
</div>
<p>We factored polynomial <span class="math">x^{10} - 1</span> starting with the highest level and ending on the lowest
level. On L3 we used an expression to construct the polynomial and we did not have to pay any
attention to the details, which were figured out automatically by <tt class="xref docutils literal"><span class="pre">factor_list()</span></tt> function.
On L2 we created a polynomial explicitly by providing a generator and coefficient domain. This
time we used <tt class="xref docutils literal"><span class="pre">factor_list()</span></tt> method of <tt class="xref docutils literal"><span class="pre">Poly</span></tt>. This computation took less than half
of time that was needed to compute the same thing on L3. Next we performed the same computation
on L1 level, the first internal level of the module, constructing the polynomial by providing
an explicit polynomial representation (dense in this case) and we gained a 10% speedup. Finally
we computed the factorization on the lowest level, gaining tiny speed improvement.</p>
<p>We can see that, depending on the level on which we performed the computation, we had to pay
increasingly more attention to the technical details, but we gained speed improvement thanks
to this. The improvement might not seem very encouraging, especially when we compare levels
L2, L1 and L0. However, we have to keep in mind that those milli&#8211; or microseconds that we
save with each computation, have to be multiplied by the number of all computations we do,
and from this perspective we do not save only fractions of seconds but we save seconds or
even minutes or hours of computation time.</p>
</div>
<div class="section" id="the-zeroth-level-l0">
<h3>The zeroth level: L0<a class="headerlink" href="#the-zeroth-level-l0" title="Permalink to this headline">¶</a></h3>
<p>This is the lowest level of polynomials manipulation functionality, which is used only for internal
purpose of the module. Most algorithms that the module implements, especially those which are most
commonly used as parts of other algorithms or are most computationally demanding, are implemented
on this level. This includes algorithms for polynomial arithmetics, GCD and LCM computation,
square&#8211;free decomposition, polynomial factorization, root isolation, Gröbner bases and others.</p>
<p>To reduce the overhead of Python to minimum, the zeroth level is implemented in purely procedural
style: it consists only of functions and all data is passed explicitly via arguments to functions.
On this level we do not take advantage of any runtime magic like context managers, everything is
done explicitly. Besides being fast, this has also the benefit that the code is very verbose and
thus easily understandable, which is not often the case in object&#8211;oriented programming. It might
seem a bit awkward, after so many years of declining of procedural programming, to use this old
fashioned style, but currently it seems the right choice. Although, in the opinion of the author,
procedural code is easy to understand, it is not that easy to write, especially for newcomers,
because of very high level of verboseness of such code. However, this is the trade&#8211;off we have
to make, to make SymPy both usable and reasonably fast.</p>
<p>Functions on this level are spread over several source files in <tt class="docutils literal"><span class="pre">sympy/polys</span></tt> and are split into
groups, depending on which polynomial representation they belong to and what kind of ground domain
can be used with them. Currently there are four major groups, which can be distinguished by their
special prefixes:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">gf_</span></tt> &#8212; dense univariate polynomials over finite (Galois) fields</li>
<li><tt class="docutils literal"><span class="pre">dup_</span></tt> &#8212; dense univariate polynomials over arbitrary domains</li>
<li><tt class="docutils literal"><span class="pre">dmp_</span></tt> &#8212; dense multivariate polynomials over arbitrary domains</li>
<li><tt class="docutils literal"><span class="pre">sdp_</span></tt> &#8212; sparse distributed polynomials over arbitrary domains</li>
</ul>
<p>There are additional minor suffixes, which can be added to the major prefixes. They are used to tell
the difference between the same function, e.g. for computing the greatest common divisor, for various
ground domains. The typically used suffixes are <tt class="docutils literal"><span class="pre">zz_</span></tt>, <tt class="docutils literal"><span class="pre">qq_</span></tt>, <tt class="docutils literal"><span class="pre">rr_</span></tt> and <tt class="docutils literal"><span class="pre">ff_</span></tt>, which stand
for the ring of integers, the rational field, a ring and a field, respectively. Note that those
suffixes are not combined with the <tt class="docutils literal"><span class="pre">gf_</span></tt> prefix, which already limits the possible ground domains
to a single one. Usually, if a function comes with a suffix, then it will be defined for both, either
<tt class="docutils literal"><span class="pre">zz_</span></tt> and <tt class="docutils literal"><span class="pre">qq_</span></tt>, or <tt class="docutils literal"><span class="pre">rr_</span></tt> and <tt class="docutils literal"><span class="pre">ff_</span></tt> suffixes. There will be also a function without any suffix,
which will dispatch the flow to an appropriate function for a specialized ground domain, depending on
the analysis of the ground domain argument to this function. This separation is necessary, because it
often happens that functions for computing a particular thing over different ground domains, have very
different semantics and internal structure. A good examples are functions for computing GCDs and
factorizations of polynomials. One should also note that even if there is a separation between different
ground domains, it is still possible (and it often happens) that a function for a more general domain
will transform the problem and run an algorithm for a smaller domain, to take advantage of more efficient
algorithms.</p>
<p>Although we listed four groups of types of functions, there are really only three true groups, because
<tt class="docutils literal"><span class="pre">dup_</span></tt> and <tt class="docutils literal"><span class="pre">dmp_</span></tt> depend on each other, forming a larger group. This is because <tt class="docutils literal"><span class="pre">dmp_</span></tt> functions
use <tt class="docutils literal"><span class="pre">dup_</span></tt> function to terminate recurrence, as <tt class="docutils literal"><span class="pre">dmp_</span></tt> implement dense recursive representation.</p>
<p>Suppose we want to implement a function for computing Taylor shifts. Given a univariate polynomial
<span class="math">f</span> in <span class="math">\K[x]</span>, where <span class="math">\K</span> is an arbitrary domain, and a value <span class="math">a \in \K</span>, we call a Taylor shift an
evaluation of <span class="math">f(x + a)</span>. For details of the algorithm refer to <a class="reference external" href="literature.html#nijenhuis1978combinatorial">[Nijenhuis1978combinatorial]</a>. To focus
our attention, we will show a sample implementation Taylor shift algorithm only for dense polynomial
representation. The implementation may be as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nd">@cythonized</span><span class="p">(</span><span class="s">&#39;n,i,j&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dup_taylor</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluate efficiently Taylor shift ``f(x + a)`` in ``K[x]``. &quot;&quot;&quot;</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="n">dup_degree</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
            <span class="n">f</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="o">*</span><span class="n">f</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">f</span>
</pre></div>
</div>
<p>We defined a function called <tt class="docutils literal"><span class="pre">dup_taylor</span></tt> which takes three arguments: <tt class="docutils literal"><span class="pre">f</span></tt>, <tt class="docutils literal"><span class="pre">a</span></tt> and <tt class="docutils literal"><span class="pre">K</span></tt>. We
followed here the standard convention of L0 level, where <tt class="docutils literal"><span class="pre">dup_</span></tt> prefix tells us that the function
uses dense polynomial representation and allows only univariate polynomials. In the arguments list,
the first argument is an input polynomial, the second is evaluation point <span class="math">a</span> and the last one is
the coefficient domain. If a function requires two polynomials as input, e.g. multiplication function,
then the first two arguments are polynomials and other arguments come next. However, the domain is
always the last one (not counting optional or keyword&#8211;only arguments, which are rarely used on
this level).</p>
<p>This was for dense univariate polynomials. The convention for other groups of functions varies a
little bit. In the case of dense multivariate polynomials we add an additional argument <tt class="docutils literal"><span class="pre">u</span></tt>, which
always goes before the ground domain, and stands for the number of variables of the input polynomial
minus one. Thus we get zero for univariate polynomials, which is convenient, because in the univariate
case, we can efficiently check that the polynomial is really univariate and fallback to the equivalent
univariate function to terminate recurrence. In the case of sparse distributed polynomials we add
another argument <tt class="docutils literal"><span class="pre">O</span></tt>, besides <tt class="docutils literal"><span class="pre">u</span></tt>, which goes between <tt class="docutils literal"><span class="pre">u</span></tt> and <tt class="docutils literal"><span class="pre">K</span></tt>, and stands for a function
that defines an ordering relation between monomials (more about monomial orderings can be found in
section <a class="reference external" href="groebner.html#thesis-orderings"><em>Admissible orderings of monomials</em></a>).</p>
<p>A very special case is the group of <tt class="docutils literal"><span class="pre">gf_</span></tt> functions, which are used for computations with polynomials
over finite fields. The convention in this case is that the domain <tt class="docutils literal"><span class="pre">K</span></tt> is some domain representing
the ring of integers, not finite fields. We add an additional argument <tt class="docutils literal"><span class="pre">p</span></tt>, which goes before <tt class="docutils literal"><span class="pre">K</span></tt>
and stands for the size of the finite field (modulus). This, somehow awkward convention, has purely
historical background, because when L0 level was invented there were no ground domains in SymPy, so
the argument <tt class="docutils literal"><span class="pre">p</span></tt> was the way to pass knowledge about the finite field, in which computations are
done, to <tt class="docutils literal"><span class="pre">gf_</span></tt> function. When ground domains were added to SymPy, then it was too costly at that
time to remove argument <tt class="docutils literal"><span class="pre">p</span></tt> and use finite field domains instead of integer ring domains. It was
also uncertain if such move would not add too much overhead and slowdown <tt class="docutils literal"><span class="pre">gf_</span></tt> functions. A study
is needed to show if there is any advantage at all of having separate group of functions for the
special case of finite fields. Possibly in near future <tt class="docutils literal"><span class="pre">gf_</span></tt> functions will be merged with <tt class="docutils literal"><span class="pre">dup_</span></tt>
functions, and <tt class="docutils literal"><span class="pre">gf_</span></tt> will be transformed to a suffix, because still finite fields are special
because specialized algorithms are needed for performing computations over this domain.</p>
</div>
<div class="section" id="the-first-level-l1">
<h3>The first level: L1<a class="headerlink" href="#the-first-level-l1" title="Permalink to this headline">¶</a></h3>
<p>This is the second level in polynomials manipulation module and the last level used for internal
purpose. It is implemented in object&#8211;oriented style and wraps up functionality of the lowest
level into four classes: <tt class="docutils literal"><span class="pre">GFP</span></tt>, <tt class="docutils literal"><span class="pre">DUP</span></tt>, <tt class="docutils literal"><span class="pre">DMP</span></tt> and <tt class="docutils literal"><span class="pre">SDP</span></tt>. Each class has methods which
reflect functions of L0 level, but with prefixes stripped. Also method call convention changes,
because ground domain and other properties are included in instances of L1 classes and provided
only on class initialization. This makes usage of functionality exposed by this level more
efficient, because the code is not that verbose as on the lowest level. L1 also implements
several other classes which provide more general computational tools, e.g. <tt class="docutils literal"><span class="pre">DMF</span></tt> for dense
multivariate fractions and <tt class="docutils literal"><span class="pre">ANP</span></tt> for a representation of algebraic numbers.</p>
<p>Classes of L1 level add tiny overhead over L0 functions, because L1 allows for unification
polynomials if they have different ground domains and there is additional constant time
needed to construct instances of L1 classes. In general in SymPy we allow only immutable
classes, so instantiation overhead is added with every computation.</p>
<p>The main task for L1 level, besides wrapping up functionality of the lowest level, is to
provide types (classes) which can be used in composite ground domains, i.e. polynomial,
rational function and algebraic domains. We could use for this purpose tools of levels L2
and L3, but overhead associated with them would be too significant and thus computations
with composite ground domains would be very inefficient. This way levels L0 and L1 define
a self contained computational model for polynomials, which is later wrapped in much more
user friendly levels L2 and L3, which we will describe next.</p>
</div>
<div class="section" id="the-second-level-l2">
<h3>The second level: L2<a class="headerlink" href="#the-second-level-l2" title="Permalink to this headline">¶</a></h3>
<p>This is the first level in polynomials manipulation module that is oriented towards the end user.
We implement only one class in L2, <tt class="xref docutils literal"><span class="pre">Poly</span></tt>, which wraps up by composition <tt class="docutils literal"><span class="pre">GFP</span></tt>, <tt class="docutils literal"><span class="pre">DUP</span></tt>,
<tt class="docutils literal"><span class="pre">DMP</span></tt> and <tt class="docutils literal"><span class="pre">SDP</span></tt> classes of L1, which we call, in this setup, polynomial representations. The
class of L2 implements the union of all methods that are available in L1 classes. If certain
operation is not supported by an underlying representation, <tt class="xref docutils literal"><span class="pre">OperationNotSupported</span></tt> exception
is raised. Otherwise, during a method call of <tt class="xref docutils literal"><span class="pre">Poly</span></tt> high&#8211;level input arguments are converted
to lower&#8211;level representations and passed to L1 level. When L1 finishes, output is converted back
to high&#8211;level representations.</p>
<p><tt class="xref docutils literal"><span class="pre">Poly</span></tt> implements expression parser, which allows to construct polynomials not only from
raw polynomial representations, e.g. a list of coefficients, but it can also parse SymPy&#8216;s
expressions and translate them to a desired polynomial representation. It is also possible to
automatically derive as much information as possible about an expression, without forcing the
user to provide additional information manually. For example <tt class="xref docutils literal"><span class="pre">Poly</span></tt> can figure out the
generators and the domain of expression on its own. This is very useful functionality, especially
in interactive sessions, because it allows to significantly cut on typing.</p>
</div>
<div class="section" id="the-third-level-l3">
<h3>The third level: L3<a class="headerlink" href="#the-third-level-l3" title="Permalink to this headline">¶</a></h3>
<p>To make the functionality, provided by the module, more accessible by the user, most methods of
<tt class="xref docutils literal"><span class="pre">Poly</span></tt> are exposed to the top&#8211;level via global functions of L3 level. Those functions
allow to use procedural interface to the module, making it appealing to users, who have already
some experience in other symbolic mathematics software. There are also some additional functions,
which wrap more general functions and allow to take advantage of their behaviour in some specific
way. For example, there is a method of L2 and a function of L3, <tt class="xref docutils literal"><span class="pre">factor_list()</span></tt>, which returns
a list of irreducible factors of a polynomial. L3 implements an additional function <tt class="xref docutils literal"><span class="pre">factor()</span></tt>
which uses <tt class="xref docutils literal"><span class="pre">factor_list()</span></tt> to compute factorization of a polynomial, but instead of returning
a list, it returns an expression in factored (multiplicative) form.</p>
<p>The <tt class="xref docutils literal"><span class="pre">Poly</span></tt> and all functions exposed by L3 level are called the public API of polynomials
manipulation module. The public API is the outcome of <tt class="docutils literal"><span class="pre">from</span> <span class="pre">sympy.polys</span> <span class="pre">import</span> <span class="pre">*</span></tt> statement. It
is, however, not an issue to import other functions and classes from the module. The user should
be aware that only the public API implements user&#8211;friendly interface and using lower&#8211;level tools
might be a pain, for example in interactive sessions. To cut on redundancy, the reader should refer
to section <a class="reference internal" href="#thesis-struct-motivation"><em>Motivation</em></a> to see example of the same computation done on different
levels of polynomials manipulation module. During the typical usage of SymPy, only the public API,
i.e. levels L2 and L3, are be necessary.</p>
</div>
<div class="section" id="multiple-levels-in-practice">
<h3>Multiple&#8211;levels in practice<a class="headerlink" href="#multiple-levels-in-practice" title="Permalink to this headline">¶</a></h3>
<p>Suppose we are given a univariate polynomial with integer coefficients:</p>
<div class="math">
f = 3 x^{17} + 3 x^5 - 20 x^2 + x + 17</div>
<p>We ask what is the value of <span class="math">f</span> at some specific point, say <span class="math">15</span>. We can achieve this by
substituting <span class="math">15</span> for <span class="math">x</span> using <tt class="xref docutils literal"><span class="pre">subs()</span></tt> method of <tt class="docutils literal"><span class="pre">Basic</span></tt> (the root class in SymPy).
This might not seem to be the optimal solution for the problem, because <tt class="xref docutils literal"><span class="pre">subs()</span></tt> does not
take advantage of the structure of an input expression, just applies blindly pattern matching
and SymPy&#8217;s built&#8211;in evaluation rules. This is, however, the first approach we can come out
with, so lets try it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">17</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">20</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">17</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="o">.</span><span class="n">subs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="go">295578376007082351782</span>
</pre></div>
</div>
<p>As the solution we obtained a very large number which is greater than <span class="math">2^{64}</span>, i.e. can&#8217;t fit
into CPU registers of modern machines. This is not an issue, because SymPy reuses Python&#8217;s
arbitrary length integers, which are only bounded by the size of available memory. The size
of the computed value might, however, raise a question concerning the speed of evaluation. As
we said, <tt class="xref docutils literal"><span class="pre">subs()</span></tt> is a very naive function. Lets see how fast it can be:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">f</span><span class="o">.</span><span class="n">subs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">15</span><span class="p">);</span>
<span class="go">1000 loops, best of 3: 992 us per loop</span>
</pre></div>
</div>
<p>It takes <tt class="xref docutils literal"><span class="pre">subs()</span></tt> about one millisecond to compute <span class="math">f(15)</span>. This seems not that bad at all,
especially in an interactive session, because one millisecond is not measurable about of time
for the user. Lets check if we get the same behaviour for much larger evaluation points:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">f</span><span class="o">.</span><span class="n">subs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">15</span><span class="o">**</span><span class="mi">20</span><span class="p">);</span>
<span class="go">1000 loops, best of 3: 1.03 ms per loop</span>
</pre></div>
</div>
<p>We chose a relatively large number <span class="math">15^{20}</span> for this test and obtained increase in evaluation
time by a very small fraction. This is still fine in an interactive session, but would it be
acceptable if <tt class="xref docutils literal"><span class="pre">subs()</span></tt> was used as a component of another algorithm, which requires several
thousandths of evaluations? Lets consider a more demanding example. We now use <tt class="xref docutils literal"><span class="pre">random_poly()</span></tt>
function to generate a polynomial of large degree to see how <tt class="xref docutils literal"><span class="pre">subs()</span></tt> scales when the size of
the problem increases:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">g</span> <span class="o">=</span> <span class="n">random_poly</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">time</span> <span class="n">g</span><span class="o">.</span><span class="n">subs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">15</span><span class="p">);</span>
<span class="go">CPU times: user 7.03 s, sys: 0.00 s, total: 7.03 s</span>
<span class="go">Wall time: 7.20 s</span>
</pre></div>
</div>
<p>This time the results are not encouraging at all. We got <span class="math">7</span> seconds of evaluation time, for a
polynomial of the degree <span class="math">1000</span> with integer coefficients bounded by <span class="math">-10</span> and <span class="math">10</span>. This is not
acceptable even in an interactive session. It gets even worse if we do the computation using the
larger evaluation point:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">time</span> <span class="n">g</span><span class="o">.</span><span class="n">subs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">15</span><span class="o">**</span><span class="mi">20</span><span class="p">);</span>
<span class="go">CPU times: user 9.88 s, sys: 0.04 s, total: 9.91 s</span>
<span class="go">Wall time: 10.29 s</span>
</pre></div>
</div>
<p>Can we do better than this? To improve this timing we need to take advantage of the structure
of the input expressions, i.e. recognize that both <span class="math">f</span> and <span class="math">g</span> are univariate polynomial with
a very simple kind of coefficients &#8212; integers. This knowledge is very important, because we
can pick up an optimized algorithm for this particular domain of computation. A well known
algorithm for evaluating univariate polynomial is Horner&#8217;s scheme, which is implemented in
<a title="(in Python v2.6)" class="reference external" href="http://docs.python.org/library/functions.html#eval"><tt class="xref docutils literal"><span class="pre">eval()</span></tt></a> method of <tt class="xref docutils literal"><span class="pre">Poly</span></tt> class. Lets rewrite <span class="math">f</span> and <span class="math">g</span> as polynomials and redo
the timings:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">G</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">F</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="mi">15</span><span class="p">);</span>
<span class="go">10000 loops, best of 3: 34.3 us per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">F</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="mi">15</span><span class="o">**</span><span class="mi">20</span><span class="p">);</span>
<span class="go">10000 loops, best of 3: 43.1 us per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">G</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="mi">15</span><span class="p">);</span>
<span class="go">1000 loops, best of 3: 1.02 ms per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">G</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="mi">15</span><span class="o">**</span><span class="mi">20</span><span class="p">);</span>
<span class="go">100 loops, best of 3: 16.4 ms per loop</span>
</pre></div>
</div>
<p>We used <tt class="xref docutils literal"><span class="pre">Poly</span></tt> to obtain polynomials form of <span class="math">f</span> and <span class="math">g</span>, arriving with polynomials <span class="math">F</span>
and <span class="math">G</span> respectively. We can clearly see, especially in the case of the large degree polynomial,
that <a title="(in Python v2.6)" class="reference external" href="http://docs.python.org/library/functions.html#eval"><tt class="xref docutils literal"><span class="pre">eval()</span></tt></a> introduced a significant improvement in execution times. This is not an
accident, because <a title="(in Python v2.6)" class="reference external" href="http://docs.python.org/library/functions.html#eval"><tt class="xref docutils literal"><span class="pre">eval()</span></tt></a> uses a dedicated algorithm for the task and, what is currently
not visible, takes advantage of gmpy library, a very efficient library for doing integer
arithmetics. This may be considered as a cheat, so lets force <tt class="xref docutils literal"><span class="pre">Poly</span></tt> to compute with
SymPy&#8217;s built&#8211;in integer type:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sympy.polys.algebratools</span> <span class="kn">import</span> <span class="n">ZZ_sympy</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">FF</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="n">ZZ_sympy</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">GG</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="n">ZZ_sympy</span><span class="p">())</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">FF</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="mi">15</span><span class="p">);</span>
<span class="go">1000 loops, best of 3: 226 us per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">FF</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="mi">15</span><span class="o">**</span><span class="mi">20</span><span class="p">);</span>
<span class="go">1000 loops, best of 3: 283 us per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">GG</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="mi">15</span><span class="p">);</span>
<span class="go">100 loops, best of 3: 15.7 ms per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">GG</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="mi">15</span><span class="o">**</span><span class="mi">20</span><span class="p">);</span>
<span class="go">10 loops, best of 3: 123 ms per loop</span>
</pre></div>
</div>
<p>We obtained a visible slowdown, but we are still much faster that when using <tt class="xref docutils literal"><span class="pre">subs()</span></tt>.
A careful reader would argue that those timings are cheating once again, because we did not
take in to account the construction times of <tt class="xref docutils literal"><span class="pre">Poly</span></tt> class instances. Lets check if
this is significant:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">Poly</span><span class="p">(</span><span class="n">f</span><span class="p">);</span>
<span class="go">100 loops, best of 3: 6.54 ms per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">Poly</span><span class="p">(</span><span class="n">g</span><span class="p">);</span>
<span class="go">1 loops, best of 3: 1.79 s per loop</span>
</pre></div>
</div>
<p>Indeed, construction of polynomials in this setup seems a very time consuming procedure. In
the case of the expression <span class="math">f</span> we do even worse that when using <tt class="xref docutils literal"><span class="pre">subs()</span></tt> alone. In the
later case we are still better, but the difference is not that impressive anymore. Although
this timing might look fine to the reader, it is a complete non&#8211;sense in this comparison.
This is because <tt class="xref docutils literal"><span class="pre">Poly</span></tt> class constructor expands the input expression by default and
this step takes majory of initialization time:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">G</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="go">1000 loops, best of 3: 209 us per loop</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">timeit</span> <span class="n">G</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="go">10 loops, best of 3: 20.2 ms per loop</span>
</pre></div>
</div>
<p>We know that <span class="math">f</span> and <span class="math">g</span> are already in the expanded form, so we can safely set <tt class="docutils literal"><span class="pre">expand</span></tt>
option to <tt class="xref docutils literal"><span class="pre">False</span></tt> and completely skip the expansion step. This gives us a significant
speedup, when compared to the previous timing. The reader should not get distracted by
this issue, because the reason for which <tt class="xref docutils literal"><span class="pre">expand()</span></tt>, a function which is used for
expanding expressions, is so slow, is because of its bulky implementation, which will
change in near future. Thus, there will be no need to bother with <tt class="docutils literal"><span class="pre">expand</span></tt> option.</p>
<p>Why <a title="(in Python v2.6)" class="reference external" href="http://docs.python.org/library/functions.html#eval"><tt class="xref docutils literal"><span class="pre">eval()</span></tt></a> function of <tt class="xref docutils literal"><span class="pre">Poly</span></tt> is so fast? One reason we already know, it
uses an optimized algorithm for its task and it takes advantage over very fast integers.
However, this is only a part of the story. The other is its implementation. <a title="(in Python v2.6)" class="reference external" href="http://docs.python.org/library/functions.html#eval"><tt class="xref docutils literal"><span class="pre">eval()</span></tt></a>
is implemented on the lowest level &#8212; L0. There are no overheads that are associated
with computations with symbolic expressions, thus we obtain very short execution times.</p>
</div>
</div>
<div class="section" id="polynomial-representations">
<h2>Polynomial representations<a class="headerlink" href="#polynomial-representations" title="Permalink to this headline">¶</a></h2>
<p>In the previous section we discussed the main feature of polynomials manipulation module &#8212;
the multiple&#8211;level architecture. During this discussion we introduced term <em>polynomial
representations</em>, however, we did not define it properly. Now we will fix this issue.</p>
<p>Raw polynomial representation is a data structure, e.g. list, dictionary, which holds complete
information about the structure of a polynomial and all its coefficients. Metadata, e.g. the
ground domain to which coefficients belong, it not considered as a part of a raw polynomial
representation, however, with non&#8211;raw polynomial representations &#8212; classes of L1 level.
Thus we will encounter raw polynomial representations on L0 and L1, and non&#8211;raw polynomial
representations on L1 and L2 levels. In future we will skip non&#8211;raw, simplifying the term.</p>
<p>SymPy implements two major polynomial representations: dense and sparse. Two representations
are needed because there are different classes of polynomials that can be encountered in
polynomial related problems and, depending on the choice of representation, computations can
be either fast or slow. Thus the right choice of polynomial representation for a particular
problem is significant to obtain satisfactory level of computations speed. We will see this
clearly in benchmarks at the end of this section.</p>
<div class="section" id="dense-polynomial-representation">
<h3>Dense polynomial representation<a class="headerlink" href="#dense-polynomial-representation" title="Permalink to this headline">¶</a></h3>
<p>It is worthwhile to define dense polynomial representation only for univariate polynomials.
In this case the representation of a polynomial of degree <span class="math">n</span> is a list with <span class="math">n+1</span> elements:</p>
<div class="math">
\left[\mbox{coeff}_n, \ldots, \mbox{coeff}_1, \mbox{coeff}_0\right]</div>
<p>Those elements are all coefficients, including zeros, of a polynomial, in order from the term
with the highest degree (<span class="math">\mbox{coeff}_n \cdot x^n</span>) to the constant term (<span class="math">\mbox{coeff}_0</span>).
Thus, if we ask for the leading coefficients (or monomial, or term), then this is always the first
element of dense univariate polynomial representation. If a polynomial is the zero polynomial, i.e.
it has negative degree, then the representation is simply the empty list. This is useful attitude,
because we can check very efficiently if we obtained the zero polynomial during computations. Dense
univariate representation is a very important tool in applications were all or most terms of polynomials
are non&#8211;zero. This behaviour happens when computing with special polynomials, e.g. truncated power
series or orthogonal polynomials.</p>
<p>Defining a true dense multivariate polynomial representation  is a non&#8211;sense, because the numer
of terms of a dense multivariate polynomial is huge. Lets consider a completely dense polynomial
in <span class="math">k</span> variables of total degree <span class="math">n = n_1 + \ldots + n_k</span>. Then the number of terms of this
polynomial is as large as <span class="math">\frac{(n + k)!}{n! k!}</span>. Suppose <span class="math">k = 5</span> and <span class="math">n = 50</span>, assuming
that coefficients are native 32&#8211;bit integers and are stored in an array, then we would need
almost 80 GiB of memory to hold this kind of polynomial in a true dense representation. In
practise, completely dense multivariate polynomials are rarely encountered, thus we do not
have to care about this special case.</p>
<p>However, as we have dense univariate representation, it would be convenient to somehow extend it
to the multivariate case. This can be done by introducing dense recursive representation, where
coefficients <span class="math">\mbox{coeff}_n</span>, <span class="math">\ldots</span>, <span class="math">\mbox{coeff}_0</span> are themselves dense polynomials. For
example, if they are univariate, then we obtain a bivariate polynomial altogether. This way, all
algorithms implemented for the univariate case generalize to the multivariate case by replacing
coefficient arithmetics with dense polynomial arithmetics. To terminate recurrence the univariate
case is used.</p>
<p>Dense recursive polynomial representation proved very useful during development of algorithms of
polynomials manipulation module for the multivariate case. It happens that many of them are very
easily expressed in terms of recursive function invocations. The unfortunate thing is that dense
multivariate representation suffers from a nasty behaviour when the number of variable is getting
big. The more variables there are the more sparse a polynomial is, thus its representation is
growing fast, taking a lot of storage but also requiring significant amount of time to be spent
on traversal of the data structure. This makes dense recursive representation a non&#8211;acceptable
solution for sparse computations in many variables, which are actually the most common ones in
symbolic mathematics.</p>
</div>
<div class="section" id="sparse-polynomial-representation">
<h3>Sparse polynomial representation<a class="headerlink" href="#sparse-polynomial-representation" title="Permalink to this headline">¶</a></h3>
<p>To solve the problem with rapid grow up of dense polynomial representation data structure in
the sparse case, we need to introduce sparse polynomial representation. To achieve this, we
could simply modify recursive representation and replace list of lists data structure with
dictionary of dictionaries. This would help to reduce memory footprint, but it would still
suffer from data structure long traversal times.</p>
<p>A remedy for both those problems is a non&#8211;recursive representation which stores all terms
with non&#8211;zero coefficients as a list of tuples:</p>
<div class="math">
\left[\left(\mbox{monom}_n, \mbox{coeff}_n\right), \ldots, \left(\mbox{monom}_0, \mbox{coeff}_0\right)\right]</div>
<p>where <span class="math">\mbox{monom}_i</span>, for <span class="math">i \in \{0, \ldots, n\}</span>, is a tuple consisting of exponents of
all variables &#8212; a monomial; and <span class="math">\mbox{coeff}_i</span> is the coefficient which stands towards
the <span class="math">i</span>&#8211;th monomial. This is called sparse distributed polynomial representation, but as
we have only one sparse representation, we usually skip distributed in its name.  We can
not use dictionaries instead of lists, because dictionaries are unordered in Python and
we would have to suffer from linear access to terms, e.g. the leading term.  To avoid this
odd and slow behaviour, we lists to store sparse representation and we keep their elements
ordered. This is an additional but insignificant cost because often we case replace sorting
by bisection algorithm and soring is fast in Python because it is implemented in the
interpreter (on C level).</p>
<p>As we use soring, we can provide different comparison functions (or key functions) to
customize the output of soring algorithm. This way we can have different orderings of
terms of a single polynomial, leading to different behaviour of certain methods, for
example the Gröbner bases methods, which is the biggest beneficent of this feature
(more on this will be said in section <a class="reference external" href="groebner.html#thesis-orderings"><em>Admissible orderings of monomials</em></a>). The reader should note
that in dense recursive representation there is a fixed ordering in all cases &#8212; the
lexicographic ordering.</p>
<p>Sparse polynomial representation is currently used mainly when computing with Gröbner
bases and otherwise is treated as an auxiliary data structure, and the default is dense
recursive representation. As we will see in the following section, this will have to be
changed, because sparse representation is superior to the dense one.</p>
</div>
<div class="section" id="benchmarking-polynomial-representations">
<h3>Benchmarking polynomial representations<a class="headerlink" href="#benchmarking-polynomial-representations" title="Permalink to this headline">¶</a></h3>
<p>So far we described dense and sparse polynomial representations, and we said that the
other representation should be the default one in SymPy, because it is just better
than the former one. Lets now prove that this is actually the case. For this purpose
we will construct three polynomials: 100% dense, 50% dense and sparse. To construct
the first one we will use <tt class="xref docutils literal"><span class="pre">monomials()</span></tt> function, which generates all monomials
in the given variables up to the given total degree. To make the timings feasible and
to prove a point, we will use three variables <span class="math">x,y,z</span> of total degree also three. The
50% dense polynomial will be generated by taking odd terms of the former polynomial,
given that the ordering of monomials in the lexicographic one. Sparse polynomial will
be simply the sum of <span class="math">x,y,z</span> variables. For the benchmark we exponentiated all three
polynomials for various exponents. Results of execution time measurements were collected
in plots of figures <a class="reference internal" href="#fig-100-dense-power"><em>Benchmark: Exponentiation of 100% dense polynomial in  of total degree 3</em></a>, <a class="reference internal" href="#fig-50-dense-power"><em>Benchmark: Exponentiation of 50% dense polynomial in  of total degree 3</em></a> and
<a class="reference internal" href="#fig-sparse-power"><em>Benchmark: Exponentiation of sparse polynomial </em></a>.</p>
<div align="center" class="figure" id="fig-100-dense-power">
<img alt="../images/100-dense-power.png" src="../images/100-dense-power.png" />
<p class="caption">Benchmark: Exponentiation of 100% dense polynomial in <span class="math">x,y,z</span> of total degree 3</p>
</div>
<div align="center" class="figure" id="fig-50-dense-power">
<img alt="../images/50-dense-power.png" src="../images/50-dense-power.png" />
<p class="caption">Benchmark: Exponentiation of 50% dense polynomial in <span class="math">x,y,z</span> of total degree 3</p>
</div>
<div align="center" class="figure" id="fig-sparse-power">
<img alt="../images/sparse-power.png" src="../images/sparse-power.png" />
<p class="caption">Benchmark: Exponentiation of sparse polynomial <span class="math">x + y + z</span></p>
</div>
<p>From those measures we can clearly see that sparse representation should be used as the main
polynomial representation in near future, because it works better sparse and semi&#8211;dense inputs,
thus covers most polynomials that can be encountered in real&#8211;life problems of symbolic mathematics.
However, one should not go into a conclusion that dense representation is completely useless, because,
as we already said, there are application in which dense polynomials can be found and thus SymPy and
its users can still benefit from it.</p>
</div>
</div>
<div class="section" id="categories-domains-and-ground-types">
<span id="thesis-ground"></span><h2>Categories, domains and ground types<a class="headerlink" href="#categories-domains-and-ground-types" title="Permalink to this headline">¶</a></h2>
<p>To understand and use the properties of the coefficient domain (also known as domain of computation
or ground domain, following Axiom&#8217;s naming convention) we need to somehow extract information about
the common nature of all coefficients and store this information in some data structures. This is
crucial for optimizing speed of computations, because the more we know about the domain, the better
algorithms we can pick up for doing the computations. We can also take advantage of various data
types for doing coefficient arithmetics. In this section we ill show how we can achieve this.</p>
<p>Originally SymPy only supported expressions as coefficients and enhanced expressions arithmetics
for coefficient arithmetics. By enhanced we mean arithmetics in which we try solve zero equivalence
problem <a class="reference external" href="literature.html#richardson1997zero">[Richardson1997zero]</a> in as many cases as possible. Given an expression <span class="math">f</span>, zero equivalence
problem is the problem of determining if <span class="math">f \equiv 0</span> is true or false statement. As we know, the same
expression can be given in different forms, all of which may be equivalent, sharing one canonical form.
Zero equivalence is a very important problem, because many algorithms, for example polynomial division
algorithm, to work properly, require to determine with certainty if a coefficient is zero or not. Thus,
if we fail to recognize zero, division algorithm may run forever, which is surely no what we expect.
There are domains in which zero equivalence is trivial, for example in the ring of integers or the field
of rational numbers. There are domains in which zero equivalence problem is solvable but requires some
effort. The best example of domain which posses this property is the field of rational functions. If we
perform arithmetics in this domain and not simplify the results, coefficient will grow and we will not
be able to recognize zeros. The solution is simple &#8212; simplify the intermediate results; and this is
what we did previously. However, if we do not have <em>a prior</em> knowledge about the domain, we have to infer
this knowledge with every operation we perform and, thus, we lose a lot of precious time. At the very end
there are domains in which zero equivalence in undecidable. There are two reasons for this. A domain may
be so complex that there is no simplification algorithm that could be used to transform an expression of
this domain to zero. The other case are inexact domains, for examples fixed precision approximations of
real numbers. In this case we can force zero equivalence by setting a threshold value below which (up to
a sign) everything is treated as zero.</p>
<p>To introduce structure in the world of categories of domains that can be used as ground domains to describe
coefficients of polynomial representations, we implemented in SymPy two levels of classes for representing
knowledge about the ground domain: categories and domains. Category is an abstract class which reflects an
abstract mathematical structure, which posses some operations and properties (axioms). Category classes can
never be instantiated directly. For actual usage, domain classes were added which are specializations of
categories for different data types (ground types) which implement arithmetics and algebra. Thus we can say
that domains provide a unified, concrete API, implementing interface of their categories. For example there
is <tt class="xref docutils literal"><span class="pre">Ring</span></tt> category which captures the properties and operations of Ring mathematical structure. Then
there is <tt class="xref docutils literal"><span class="pre">IntegerRing</span></tt> category which special&#8211;cases <tt class="xref docutils literal"><span class="pre">Ring</span></tt>. Finally there are several domains,
<tt class="xref docutils literal"><span class="pre">ZZ_sympy</span></tt>, <tt class="xref docutils literal"><span class="pre">ZZ_python</span></tt> and <tt class="xref docutils literal"><span class="pre">ZZ_gmpy</span></tt>, which implement the interface of <tt class="xref docutils literal"><span class="pre">IntegerRing</span></tt>
using appropriate ground types: <tt class="docutils literal"><span class="pre">Integer</span></tt>, <tt class="docutils literal"><span class="pre">int</span></tt> and <tt class="docutils literal"><span class="pre">mpz</span></tt>, respectively.</p>
<p>Ground type is a data type which implements actual arithmetics and algebra of a domain and provides all its
functionality via some interface. Different types have different interfaces, procedural, object&#8211;oriented or
mixed. They may implement some functionality but not the other. This way direct usage of different ground
types is non&#8211;trivial, because we have to accommodate for all those differences. In SymPy this is no more
an issue, as we have domains which provide a single interface over all data types from one category. The
reader may ask why we chose to this approach. Alternatively we could create a common ground type for all
ground types from a particular category and also have a unified interface. However, this way we introduce
a very heavy layer which adds too much overhead. In our approach we make an assumptions that every ground
type has to provide some most common methods, which can later can be directly used without any additional
overhead. The observation we made in the case of integer (and also rational and inexact) ground types, is
that there is always a common core of methods implemented in each type: unary and binary operators, excluding
division. Those listed are the most often used so it is very beneficial that we can use them directly. On
the other hand, division is not that often operation so we can add minimal overhead of additional function
call, to make the interface cross&#8211;type compatible.</p>
<p>Besides all the benefits concerning zero equivalence and speed related issues, introduction of categories,
domains and ground types had one additional and very simple reason. Python is a dynamically typed programming
language, thus it not allows us to declare types of variables and type inference is done at runtime. This also
implies that the language is missing a very important feature &#8212; templates. With templates we could easily
have a single source base and code capable of running with different types of coefficients. By introducing
infrastructure of this section, we try to simulate templates in Python. Of course, our templates machinery
does its work at runtime, but the overhead is small and we can in future add some optimisations at module
initialization time, e.g. automatic generation of different versions of functions depending on the domain
of computation.</p>
<p>The idea, to model mathematical structures in SymPy this way, came after studying Aldor &#8212; a compiled
language for implementing mathematics, which has a very strong static typing engine <a class="reference external" href="literature.html#aldor2000guide">[Aldor2000guide]</a>.
There is a library for Aldor called Algebra <a class="reference external" href="literature.html#bronstein2004algebra">[Bronstein2004algebra]</a>, which implements most mathematical
structures that are needed in symbolic mathematics software. The structure of this library was an
inspiration for the design of categories and domains in SymPy.</p>
<div class="section" id="benchmarking-ground-types">
<h3>Benchmarking ground types<a class="headerlink" href="#benchmarking-ground-types" title="Permalink to this headline">¶</a></h3>
<p>Enough was said about the theory of ground types, so lets now benchmark some examples and verify
if this infrastructure gives any befits at all. We will solve two factorization problems over the
integers: one with small coefficients (<span class="math">\pm 1</span>) and the other with large coefficients (at most <span class="math">56</span>
digits). Results of this benchmark were collected in plots of figures <a class="reference internal" href="#fig-ground-factor-small"><em>Benchmark: Factorization of  (small coefficients)</em></a>
and <a class="reference internal" href="#fig-ground-factor-large"><em>Benchmark: Factorization of  (large coefficients)</em></a>.</p>
<div align="center" class="figure" id="fig-ground-factor-small">
<img alt="../images/ground-factor-small.png" src="../images/ground-factor-small.png" />
<p class="caption">Benchmark: Factorization of <span class="math">x^n - 1</span> (small coefficients)</p>
</div>
<div align="center" class="figure" id="fig-ground-factor-large">
<img alt="../images/ground-factor-large.png" src="../images/ground-factor-large.png" />
<p class="caption">Benchmark: Factorization of <span class="math">(1234 x + 123 y + 12 z + 1)^n</span> (large coefficients)</p>
</div>
<p>In the case of both benchmarks we see that computations with coefficients based on SymPy&#8216;s
<tt class="docutils literal"><span class="pre">Integer</span></tt> type are the slowest. This is because, <tt class="docutils literal"><span class="pre">Integer</span></tt> is a wrapper if Python&#8217;s <tt class="docutils literal"><span class="pre">int</span></tt>
type and, thus, it adds significant overhead. From <a class="reference internal" href="#fig-ground-factor-small"><em>Benchmark: Factorization of  (small coefficients)</em></a> we see that
<tt class="docutils literal"><span class="pre">int</span></tt> is a little better for small coefficients than gmpy&#8217;s <tt class="docutils literal"><span class="pre">mpz</span></tt> type. A reason for this
might be that attribute access for the built&#8211;in type is highly optimized, whereas <tt class="docutils literal"><span class="pre">mpz</span></tt> has
to use more general ways to access attributes. We did not investigate this, so there might be
other explanations for this behaviour. However, for large coefficients <tt class="docutils literal"><span class="pre">mpz</span></tt> is unbeatable,
because it implements asymptotically faster algorithms for integer arithmetics than <tt class="docutils literal"><span class="pre">int</span></tt> has.</p>
<p>In SymPy we use by default gmpy&#8217;s ground type, if the library is available on the system. When
this is not the case, SymPy switches to Python&#8217;s ground types. SymPy&#8216;s ground types are used
usually for benchmarks, although, in the case of obsolete Python 2.4 and 2.5, which are still
supported by SymPy, there is a need to use SymPy&#8216;s <tt class="docutils literal"><span class="pre">Rational</span></tt> type to implement rational
number domain, because <tt class="docutils literal"><span class="pre">Fraction</span></tt> type was introduced in Python 2.6.</p>
</div>
</div>
<div class="section" id="using-cython-internally">
<span id="thesis-cython"></span><h2>Using Cython internally<a class="headerlink" href="#using-cython-internally" title="Permalink to this headline">¶</a></h2>
<p>Cython (<a class="reference external" href="http://www.cython.org">http://www.cython.org</a>) is a general purpose programming language that is based on Python
(shares very similar syntax), but has extra language extensions to allow static typing and
allows for direct translation into optimized C code. Cython makes it easy to write Python
wrappers to foreign libraries for exposing exposing their functionality to interpreted code.
It also allows to optimize pure Python codes, which we take advantage of.</p>
<p>There are two approaches to enhance software speed that is written in pure Python. The first
is to rewrite carefully selected parts of Python code in Cython and compile them. Depending
on the quality of Cython code, speed gain might vary, but, in any case, will be substantial
over the pure Python version. This approach has the benefit that developers have complete
control over the optimizations that are used. However, one has to provide two source bases
for optimized parts of the system, which makes maintenance and future extensions complicated.
This makes direct Cython usage a measure of last resort when optimizing Python codes.</p>
<p>The other approach is to use, so called, <em>pure mode</em> Cython. This a very recent development
in Cython, which allows the developers to keep a single source base of pure Python code, while
still benefiting from translation to machine code level, gaining speed improvement. Single source
base is achieved by simply decorating functions or methods with a special decorator, which marks
a function or method as compilable. The decorator also allows to specify which variables will be
considered as native and which will remain pure Python variables. It is also possible to declare
the type of each native variable. Then the decorated code can be run as normally in a standard
Python interpreter, of course without any speed gain, but, what is more important, without any
speed degeneracy (Cython decorators are empty decorators in interpreted mode). To take advantage
of Cython, the user has to compile selected modules with Cython compiler, which results in a
dynamically linked library (e.g. <tt class="docutils literal"><span class="pre">*.so</span></tt> on Unix platforms and <tt class="docutils literal"><span class="pre">*.dll</span></tt> on Windows) for each
compiled module. During the next execution of the system, Python interpreters will select compiled
modules in favour to the pure Python ones.</p>
<p>It should be clearly understood that if a variable is marked as native, then it conforms to the
rules of the platform for which the code was compiled. If we declared, for example, a variable
to be of integer type (C&#8217;s <tt class="docutils literal"><span class="pre">int</span></tt> type), then there will be restriction of on the size of values
accepted by such variable. There is no overflow checking or automatic conversion to arbitrary
length integers, so one has to be very careful about which variables are marked as native to
avoid faulty code on certain platforms.</p>
<div class="section" id="pure-mode-cython-in-sympy">
<h3>Pure mode Cython in SymPy<a class="headerlink" href="#pure-mode-cython-in-sympy" title="Permalink to this headline">¶</a></h3>
<p>To reduce the overhead of using pure mode Cython in SymPy to minimum, <tt class="docutils literal"><span class="pre">&#64;cythonized</span></tt> decorator
was introduced, which wraps original Cython&#8217;s decorators and adjusts them to SymPy&#8217;s needs. This
is useful because we don not take advantage of many advanced Cython&#8217;s features. The only feature
we need is to mark variables as native, because all native variables in SymPy, at least at this
point, are integers, so there is not need to make things unnecessarily complicated. The decorator
also allows to run SymPy in interpreted mode without Cython installation on the system. To achieve
this, which is one of fundamental SymPy&#8217;s goals, we simply try import Cython and when this is not
possible, we simply define an empty decorator.</p>
<p>Suppose we implement a function for rising a value to the <span class="math">n</span>&#8211;th power, <tt class="xref docutils literal"><span class="pre">power()</span></tt> in this case.
For this task we employ classical repeated squaring algorithm. A sample implementation goes as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="nd">@cythonized</span><span class="p">(</span><span class="s">&#39;value,result,n,m&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">power</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Raise ``value`` to the ``n``--th power. &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">value</span>
    <span class="k">elif</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;negative exponents are not supported&quot;</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span>

        <span class="k">if</span> <span class="n">m</span> <span class="o">&amp;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">*=</span> <span class="n">value</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">n</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">value</span> <span class="o">**=</span> <span class="mi">2</span>

    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
<p>The function can be run in both interpreted and compiled modes. To tell Cython that <tt class="xref docutils literal"><span class="pre">power()</span></tt>
is ready for compilation, we use <tt class="docutils literal"><span class="pre">&#64;cythonized</span></tt> decorator, in which we declare four variables as
native: <tt class="docutils literal"><span class="pre">value</span></tt>, <tt class="docutils literal"><span class="pre">result</span></tt>, <tt class="docutils literal"><span class="pre">n</span></tt> and <tt class="docutils literal"><span class="pre">m</span></tt>. All four will have <tt class="docutils literal"><span class="pre">int</span></tt> type assigned during
translation to C code. Two of those variables are input to the function. Cython is enough clever
to automatically convert Python integers to native integers if necessary. The same can happen the
other way in <tt class="docutils literal"><span class="pre">return</span></tt> statement. It is important to note that in interpreted mode Python uses
arbitrary length integers, so we can compute arbitrary powers using <tt class="xref docutils literal"><span class="pre">power()</span></tt> function. This
situation changes in compiled mode because we are restricted by machine types &#8212; we can store
at most 32&#8211;bit or 64&#8211;bit values in native variables, depending on the actual architecture for
which this piece of code was compiled. Such difference can have serious consequences, like wrong
results of computation, if pure mode Cython was used without understanding of the behavior of this
code on different platforms. This shows that developers have to be very careful about which variables
should and which should not be marked as native. The general advice is to use native variables for
loop indexes and auxiliary storage which we can guarantee to remain in right bounds. There are,
however, cases where native variables can be used for doing actual computations, because it
might be unrealistic to use larger values that 32&#8211;bit long. A good example is <tt class="xref docutils literal"><span class="pre">divisors()</span></tt>,
which computes all divisors of an integer.</p>
<p>In polynomials manipulation mode we marked all loop index variables and some auxiliary variables on
the lowest level as native. At this moment no coefficient arithmetics is done natively. It would be,
however, very convenient in future to take advantage of native coefficient arithmetics when computing
with polynomials over finite fields. In most practical cases in SymPy, coefficients which arise over
finite fields are half&#8211;words, so arithmetics (especially including multiplication) can be done in a
single machine word (32 bits). We could also consider allowing 64&#8211;bit words, if architecture supports
this, to widen the range of application of optimized routines.</p>
<p>To take advantage of Cython in SymPy, the user has to compile it. By default SymPy ships only with
bytecode modules and scripts for compiling them, if Cython is installed on the system. Assuming
that GNU make is installed, then compiling SymPy is as simple as typing <tt class="docutils literal"><span class="pre">make</span></tt> at a shell prompt
in the main directory of SymPy source distribution. If GNU make is not available, then the user
has to issue the following command:</p>
<div class="highlight-python"><pre>python build.py build_ext --inplace</pre>
</div>
<p>from the same directory. This command tells Cython to compile all modules in SymPy, which have
functions or methods marked with <tt class="docutils literal"><span class="pre">&#64;cythonized</span></tt> decorator. The compilation is done in&#8211;place,
meaning that there will two additional files for each Python source file: <tt class="docutils literal"><span class="pre">*.pyc</span></tt> file with
bytecode and a compiled dynamically linked library. If <tt class="docutils literal"><span class="pre">--inplace</span></tt> was omitted, then Cython
would store compiled modules in a separate directory, which would make running SymPy with
compiled modules complicated.</p>
</div>
<div class="section" id="benchmarking-pure-mode-cython">
<h3>Benchmarking pure mode Cython<a class="headerlink" href="#benchmarking-pure-mode-cython" title="Permalink to this headline">¶</a></h3>
<p>It is very cheap to employ pure mode Cython in Python code. Does it, however, bring any improvement
over pure Python? First experiments with pure mode Cython, which we conducted in SymPy, showed that
the befit can be substantial or even impressive, giving over 20 times speedup for particular small
functions, in which we could use native variables for coefficient arithmetics. The main subject of
those experiments was <tt class="xref docutils literal"><span class="pre">divisors`()</span></tt> function. Those experiments were, however, artificial and
for real&#8211;life cases speedup is not that big, but still worthy consideration, especially we take
the tiny cost of pure mode Cython (one additional line per function or method).</p>
<p>Suppose we expand a non&#8211;trivial expression <span class="math">((x + y + z)^{15} + 1) \cdot ((x + y + z)^{15} + 2)</span> and
then we want to factor the result back. We are interested only in factorization time. We perform
the same computation in pure Python and pure mode Cython:</p>
<ul>
<li><dl class="first docutils">
<dt>pure Python</dt>
<dd><div class="first last highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">expand</span><span class="p">(((</span><span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">+</span><span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">15</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">((</span><span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">+</span><span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">15</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">time</span> <span class="n">a</span> <span class="o">=</span> <span class="n">factor</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="go">CPU times: user 109.45 s, sys: 0.01 s, total: 109.47 s</span>
<span class="go">Wall time: 110.83 s</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">time</span> <span class="n">a</span> <span class="o">=</span> <span class="n">factor</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="go">CPU times: user 109.31 s, sys: 0.03 s, total: 109.34 s</span>
<span class="go">Wall time: 110.68 s</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>pure mode Cython</dt>
<dd><div class="first last highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">expand</span><span class="p">(((</span><span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">+</span><span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">15</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">((</span><span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">+</span><span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">15</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">time</span> <span class="n">a</span> <span class="o">=</span> <span class="n">factor</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="go">CPU times: user 72.09 s, sys: 1.02 s, total: 73.11 s</span>
<span class="go">Wall time: 74.18 s</span>

<span class="gp">&gt;&gt;&gt; </span><span class="o">%</span><span class="n">time</span> <span class="n">a</span> <span class="o">=</span> <span class="n">factor</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="go">CPU times: user 72.81 s, sys: 0.04 s, total: 72.85 s</span>
<span class="go">Wall time: 73.74 s</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
<p>We can see that for this very particular benchmark we obtained 1.5 times speedup. This is
not 20 times, but still can be considered important, especially when such long computation
times are involved. More throughout timings can be found in figures <a class="reference internal" href="#fig-cython-power"><em>Benchmark: Exponentiation of </em></a>
and <a class="reference internal" href="#fig-cython-factor"><em>Benchmark: Factorization of  over integers</em></a>, where we exponentiated and factored polynomials for various
exponents and degrees, respectively.</p>
<div align="center" class="figure" id="fig-cython-power">
<img alt="../images/cython-power.png" src="../images/cython-power.png" />
<p class="caption">Benchmark: Exponentiation of <span class="math">(27 x + y^2 - 15 z)^n</span></p>
</div>
<div align="center" class="figure" id="fig-cython-factor">
<img alt="../images/cython-factor.png" src="../images/cython-factor.png" />
<p class="caption">Benchmark: Factorization of <span class="math">x^n - 1</span> over integers</p>
</div>
<p>In future we expect even better improvements when native variables will be used for coefficient
arithmetics. Every algorithm which uses modular approach, which include algorithms for factoring
polynomials, computing GCD or resultants, will benefit from this, because coefficients arising
on the intermediate steps in those algorithms are usually half&#8211;words (computations are done in
small finite fields). How to achieve this, without compromising functionality and correctness,
is a subject for future discussion.</p>
</div>
</div>
<div class="section" id="conclusions">
<h2>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">¶</a></h2>
<p>In this chapter we showed how to make pure Python approach to computations with polynomials fast.
This was done in three steps, by introducing multiple&#8211;level structure, using various ground types
and taking advantage of pure mode Cython. The approach we utilized in polynomials manipulation module
was a success and was an important improvement to SymPy in general. In future we may employ similar
techniques in other parts of our library, for example to improve linear algebra module, which has
shares many similarities with polynomials module.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../static/sympy-logo.png" alt="Logo"/>
            </a></p>
            <h3><a href="../index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference external" href="#">Notes on the internal implementation</a><ul>
<li><a class="reference external" href="#physical-structure-of-the-module">Physical structure of the module</a></li>
<li><a class="reference external" href="#logical-structure-of-the-module">Logical structure of the module</a><ul>
<li><a class="reference external" href="#motivation">Motivation</a></li>
<li><a class="reference external" href="#the-zeroth-level-l0">The zeroth level: L0</a></li>
<li><a class="reference external" href="#the-first-level-l1">The first level: L1</a></li>
<li><a class="reference external" href="#the-second-level-l2">The second level: L2</a></li>
<li><a class="reference external" href="#the-third-level-l3">The third level: L3</a></li>
<li><a class="reference external" href="#multiple-levels-in-practice">Multiple&#8211;levels in practice</a></li>
</ul>
</li>
<li><a class="reference external" href="#polynomial-representations">Polynomial representations</a><ul>
<li><a class="reference external" href="#dense-polynomial-representation">Dense polynomial representation</a></li>
<li><a class="reference external" href="#sparse-polynomial-representation">Sparse polynomial representation</a></li>
<li><a class="reference external" href="#benchmarking-polynomial-representations">Benchmarking polynomial representations</a></li>
</ul>
</li>
<li><a class="reference external" href="#categories-domains-and-ground-types">Categories, domains and ground types</a><ul>
<li><a class="reference external" href="#benchmarking-ground-types">Benchmarking ground types</a></li>
</ul>
</li>
<li><a class="reference external" href="#using-cython-internally">Using Cython internally</a><ul>
<li><a class="reference external" href="#pure-mode-cython-in-sympy">Pure mode Cython in SymPy</a></li>
<li><a class="reference external" href="#benchmarking-pure-mode-cython">Benchmarking pure mode Cython</a></li>
</ul>
</li>
<li><a class="reference external" href="#conclusions">Conclusions</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="introduction.html"
                                  title="previous chapter">Introduction</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="algorithms.html"
                                  title="next chapter">Algorithms for algebraic computations</a></p>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="../sources/src/internals.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="algorithms.html" title="Algorithms for algebraic computations"
             >next</a> |</li>
        <li class="right" >
          <a href="introduction.html" title="Introduction"
             >previous</a> |</li>
        <li><a href="../index.html">Polynomials Manipulation Module v1.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
      &copy; Copyright 2010, Mateusz Paprocki.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 0.6.5.
    </div>
  </body>
</html>